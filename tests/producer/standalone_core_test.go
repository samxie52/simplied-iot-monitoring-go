package main

import (
	"testing"
	"time"
	"unsafe"

	"simplied-iot-monitoring-go/internal/config"
)

// ç›´æ¥åœ¨æµ‹è¯•ä¸­å®ç°æ ¸å¿ƒç»„ä»¶ä»¥é¿å…ä¾èµ–é—®é¢˜

// PrometheusMetrics ç®€åŒ–çš„PrometheusæŒ‡æ ‡å®ç°
type PrometheusMetrics struct {
	counters   map[string]float64
	histograms map[string][]float64
	gauges     map[string]float64
}

func NewPrometheusMetrics() *PrometheusMetrics {
	return &PrometheusMetrics{
		counters:   make(map[string]float64),
		histograms: make(map[string][]float64),
		gauges:     make(map[string]float64),
	}
}

func (pm *PrometheusMetrics) IncrementCounter(name string, labels map[string]string) {
	pm.counters[name]++
}

func (pm *PrometheusMetrics) RecordHistogram(name string, value float64, labels map[string]string) {
	pm.histograms[name] = append(pm.histograms[name], value)
}

func (pm *PrometheusMetrics) SetGauge(name string, value float64, labels map[string]string) {
	pm.gauges[name] = value
}

// MemoryPool ç®€åŒ–çš„å†…å­˜æ± å®ç°
type MemoryPool struct {
	buffers chan []byte
	size    int
}

func NewMemoryPool(bufferSize, poolSize int) *MemoryPool {
	pool := &MemoryPool{
		buffers: make(chan []byte, poolSize),
		size:    bufferSize,
	}
	
	// é¢„åˆ†é…ç¼“å†²åŒº
	for i := 0; i < poolSize; i++ {
		pool.buffers <- make([]byte, bufferSize)
	}
	
	return pool
}

func (mp *MemoryPool) Get() []byte {
	select {
	case buf := <-mp.buffers:
		return buf
	default:
		return make([]byte, mp.size)
	}
}

func (mp *MemoryPool) Put(buf []byte) {
	if len(buf) != mp.size {
		return
	}
	
	select {
	case mp.buffers <- buf:
	default:
		// æ± å·²æ»¡ï¼Œä¸¢å¼ƒç¼“å†²åŒº
	}
}

func (mp *MemoryPool) Close() {
	close(mp.buffers)
}

// ZeroCopyBuffer é›¶æ‹·è´ç¼“å†²åŒºå®ç°
type ZeroCopyBuffer struct {
	data []byte
}

func NewZeroCopyBuffer(data []byte) *ZeroCopyBuffer {
	return &ZeroCopyBuffer{data: data}
}

func (zcb *ZeroCopyBuffer) Bytes() []byte {
	return zcb.data
}

func (zcb *ZeroCopyBuffer) String() string {
	return *(*string)(unsafe.Pointer(&zcb.data))
}

func (zcb *ZeroCopyBuffer) Slice(start, end int) []byte {
	return zcb.data[start:end]
}

// é›¶æ‹·è´è½¬æ¢å‡½æ•°
func StringToBytesZeroCopy(s string) []byte {
	return *(*[]byte)(unsafe.Pointer(
		&struct {
			string
			Cap int
		}{s, len(s)},
	))
}

func BytesToStringZeroCopy(b []byte) string {
	return *(*string)(unsafe.Pointer(&b))
}

// æµ‹è¯•å‡½æ•°

func TestStandaloneConfigurationTypes(t *testing.T) {
	t.Run("AppConfig Structure", func(t *testing.T) {
		cfg := &config.AppConfig{
			App: config.AppSection{
				Name:        "test-app",
				Version:     "1.0.0",
				Environment: "test",
				Debug:       true,
				LogLevel:    "debug",
			},
			Kafka: config.KafkaSection{
				Brokers: []string{"localhost:9092"},
				Topics: config.TopicConfig{
					DeviceData: "device-data-test",
					Alerts:     "alerts-test",
				},
				Producer: config.KafkaProducer{
					ClientID:          "test-client",
					BatchSize:         100,
					BatchTimeout:      100 * time.Millisecond,
					CompressionType:   "gzip",
					MaxRetries:        3,
					RetryBackoff:      100 * time.Millisecond,
					RequiredAcks:      1,
					FlushFrequency:    50 * time.Millisecond,
					ChannelBufferSize: 256,
					Timeout:           30 * time.Second,
				},
				Consumer: config.KafkaConsumer{
					GroupID:          "test-group",
					AutoOffsetReset:  "earliest",
					EnableAutoCommit: true,
					SessionTimeout:   30 * time.Second,
					MaxPollRecords:   100,
				},
				Timeout: 30 * time.Second,
			},
			Producer: config.ProducerSection{
				DeviceCount:   10,
				SendInterval:  1 * time.Second,
				DataVariance:  0.1,
				BatchSize:     100,
				RetryAttempts: 3,
				Timeout:       30 * time.Second,
			},
			Device: config.DeviceSection{
				Simulator: config.DeviceSimulator{
					Enabled:         true,
					DeviceCount:     10,
					SampleInterval:  1 * time.Second, // æ­£ç¡®çš„å­—æ®µå
					DataVariation:   0.1,
					AnomalyRate:     0.01,
					TrendEnabled:    true,
					TrendStrength:   0.1,
					WorkerPoolSize:  4,
					QueueBufferSize: 1000,
				},
			},
			Web: config.WebSection{
				Host:           "localhost",
				Port:           8080,
				TemplatePath:   "./templates",
				StaticPath:     "./static",
				ReadTimeout:    30 * time.Second,
				WriteTimeout:   30 * time.Second,
				MaxHeaderBytes: 1024 * 1024,
			},
		}

		// éªŒè¯é…ç½®ç»“æ„çš„å®Œæ•´æ€§
		if cfg.Kafka.Topics.DeviceData == "" {
			t.Fatal("DeviceData topic not set")
		}
		if cfg.Device.Simulator.SampleInterval == 0 {
			t.Fatal("SampleInterval not set")
		}
		if cfg.Kafka.Producer.ClientID == "" {
			t.Fatal("Kafka Producer ClientID not set")
		}

		t.Log("âœ… Configuration structure validation passed")
	})
}

func TestStandaloneCoreComponents(t *testing.T) {
	t.Run("PrometheusMetrics", func(t *testing.T) {
		metrics := NewPrometheusMetrics()
		if metrics == nil {
			t.Fatal("Failed to create PrometheusMetrics")
		}

		// æµ‹è¯•åŸºæœ¬æ“ä½œ
		metrics.IncrementCounter("test_counter", map[string]string{"label": "value"})
		metrics.RecordHistogram("test_histogram", 1.5, map[string]string{"label": "value"})
		metrics.SetGauge("test_gauge", 42.0, map[string]string{"label": "value"})

		// éªŒè¯æ“ä½œç»“æœ
		if metrics.counters["test_counter"] != 1 {
			t.Fatal("Counter increment failed")
		}
		if len(metrics.histograms["test_histogram"]) != 1 {
			t.Fatal("Histogram record failed")
		}
		if metrics.gauges["test_gauge"] != 42.0 {
			t.Fatal("Gauge set failed")
		}

		t.Log("âœ… PrometheusMetrics operations completed successfully")
	})

	t.Run("MemoryPool", func(t *testing.T) {
		pool := NewMemoryPool(1024, 100)
		if pool == nil {
			t.Fatal("Failed to create MemoryPool")
		}
		defer pool.Close()

		// æµ‹è¯•å†…å­˜æ± æ“ä½œ
		buf := pool.Get()
		if buf == nil {
			t.Fatal("Failed to get buffer from pool")
		}
		if len(buf) != 1024 {
			t.Fatalf("Expected buffer size 1024, got %d", len(buf))
		}
		
		pool.Put(buf)
		
		// å†æ¬¡è·å–åº”è¯¥å¾—åˆ°ç›¸åŒçš„ç¼“å†²åŒº
		buf2 := pool.Get()
		if buf2 == nil {
			t.Fatal("Failed to get buffer from pool second time")
		}

		t.Log("âœ… MemoryPool operations completed successfully")
	})

	t.Run("ZeroCopyBuffer", func(t *testing.T) {
		data := []byte("test data for zero copy buffer")
		buf := NewZeroCopyBuffer(data)
		if buf == nil {
			t.Fatal("Failed to create ZeroCopyBuffer")
		}

		// æµ‹è¯•é›¶æ‹·è´æ“ä½œ
		bytes := buf.Bytes()
		if len(bytes) != len(data) {
			t.Fatalf("Expected %d bytes, got %d", len(data), len(bytes))
		}

		str := buf.String()
		if str != string(data) {
			t.Fatalf("Expected %s, got %s", string(data), str)
		}

		// æµ‹è¯•åˆ‡ç‰‡æ“ä½œ
		slice := buf.Slice(0, 4)
		if string(slice) != "test" {
			t.Fatalf("Expected 'test', got %s", string(slice))
		}

		t.Log("âœ… ZeroCopyBuffer operations completed successfully")
	})
}

func TestStandalonePerformance(t *testing.T) {
	t.Run("ZeroCopy Performance", func(t *testing.T) {
		data := make([]byte, 1024)
		for i := range data {
			data[i] = byte(i % 256)
		}

		// åŸºå‡†æµ‹è¯•é›¶æ‹·è´è½¬æ¢
		start := time.Now()
		iterations := 100000
		
		for i := 0; i < iterations; i++ {
			// é›¶æ‹·è´å­—ç¬¦ä¸²åˆ°å­—èŠ‚è½¬æ¢
			str := string(data)
			_ = StringToBytesZeroCopy(str)
			
			// é›¶æ‹·è´å­—èŠ‚åˆ°å­—ç¬¦ä¸²è½¬æ¢
			_ = BytesToStringZeroCopy(data)
		}
		
		duration := time.Since(start)
		avgNsPerOp := duration.Nanoseconds() / int64(iterations*2) // 2 operations per iteration
		
		t.Logf("âœ… Zero copy conversions: %d iterations in %v, avg %.2f ns/op", 
			iterations*2, duration, float64(avgNsPerOp))
		
		// éªŒè¯æ€§èƒ½ - åº”è¯¥éå¸¸å¿«ï¼ˆ< 10 ns/opï¼‰
		if avgNsPerOp > 10 {
			t.Logf("âš ï¸  Warning: Zero copy performance may be suboptimal: %.2f ns/op", float64(avgNsPerOp))
		} else {
			t.Logf("ğŸš€ Excellent zero copy performance: %.2f ns/op", float64(avgNsPerOp))
		}
	})

	t.Run("Memory Pool Performance", func(t *testing.T) {
		pool := NewMemoryPool(1024, 50)
		defer pool.Close()

		start := time.Now()
		iterations := 10000

		for i := 0; i < iterations; i++ {
			buf := pool.Get()
			pool.Put(buf)
		}

		duration := time.Since(start)
		avgNsPerOp := duration.Nanoseconds() / int64(iterations)

		t.Logf("âœ… Memory pool operations: %d iterations in %v, avg %.2f ns/op",
			iterations, duration, float64(avgNsPerOp))

		// éªŒè¯æ€§èƒ½ - åº”è¯¥åœ¨åˆç†èŒƒå›´å†…ï¼ˆ< 1000 ns/opï¼‰
		if avgNsPerOp > 1000 {
			t.Logf("âš ï¸  Warning: Memory pool performance may be suboptimal: %.2f ns/op", float64(avgNsPerOp))
		} else {
			t.Logf("ğŸš€ Good memory pool performance: %.2f ns/op", float64(avgNsPerOp))
		}
	})

	t.Run("ZeroCopyBuffer Performance", func(t *testing.T) {
		data := make([]byte, 1024)
		for i := range data {
			data[i] = byte(i % 256)
		}

		start := time.Now()
		iterations := 50000

		for i := 0; i < iterations; i++ {
			buf := NewZeroCopyBuffer(data)
			_ = buf.Bytes()
			_ = buf.String()
			_ = buf.Slice(0, 100)
		}

		duration := time.Since(start)
		avgNsPerOp := duration.Nanoseconds() / int64(iterations*3) // 3 operations per iteration

		t.Logf("âœ… ZeroCopyBuffer operations: %d iterations in %v, avg %.2f ns/op",
			iterations*3, duration, float64(avgNsPerOp))

		if avgNsPerOp > 5 {
			t.Logf("âš ï¸  Warning: ZeroCopyBuffer performance may be suboptimal: %.2f ns/op", float64(avgNsPerOp))
		} else {
			t.Logf("ğŸš€ Excellent ZeroCopyBuffer performance: %.2f ns/op", float64(avgNsPerOp))
		}
	})
}

func TestConfigurationFieldsFixed(t *testing.T) {
	t.Run("DeviceSimulator Fields", func(t *testing.T) {
		simulator := config.DeviceSimulator{
			Enabled:         true,
			DeviceCount:     10,
			SampleInterval:  1 * time.Second, // æ­£ç¡®çš„å­—æ®µå
			DataVariation:   0.1,
			AnomalyRate:     0.01,
			TrendEnabled:    true,
			TrendStrength:   0.1,
			WorkerPoolSize:  4,
			QueueBufferSize: 1000,
		}

		// éªŒè¯å­—æ®µè®¾ç½®æ­£ç¡®
		if simulator.SampleInterval != 1*time.Second {
			t.Fatal("SampleInterval not set correctly")
		}
		if simulator.DeviceCount != 10 {
			t.Fatal("DeviceCount not set correctly")
		}
		if simulator.WorkerPoolSize != 4 {
			t.Fatal("WorkerPoolSize not set correctly")
		}

		t.Log("âœ… DeviceSimulator fields validation passed")
	})

	t.Run("KafkaProducer Fields", func(t *testing.T) {
		producer := config.KafkaProducer{
			ClientID:          "test-client",
			BatchSize:         100,
			BatchTimeout:      100 * time.Millisecond,
			CompressionType:   "gzip",
			MaxRetries:        3,
			RetryBackoff:      100 * time.Millisecond,
			RequiredAcks:      1,
			FlushFrequency:    50 * time.Millisecond,
			ChannelBufferSize: 256,
			Timeout:           30 * time.Second,
		}

		// éªŒè¯å­—æ®µè®¾ç½®æ­£ç¡®
		if producer.ClientID != "test-client" {
			t.Fatal("ClientID not set correctly")
		}
		if producer.BatchSize != 100 {
			t.Fatal("BatchSize not set correctly")
		}
		if producer.Timeout != 30*time.Second {
			t.Fatal("Timeout not set correctly")
		}

		t.Log("âœ… KafkaProducer fields validation passed")
	})

	t.Run("Topic Configuration", func(t *testing.T) {
		topics := config.TopicConfig{
			DeviceData: "device-data-topic",
			Alerts:     "alerts-topic",
		}

		// éªŒè¯ä¸»é¢˜é…ç½®
		if topics.DeviceData != "device-data-topic" {
			t.Fatal("DeviceData topic not set correctly")
		}
		if topics.Alerts != "alerts-topic" {
			t.Fatal("Alerts topic not set correctly")
		}

		t.Log("âœ… Topic configuration validation passed")
	})
}

func TestSystemReadiness(t *testing.T) {
	t.Run("Configuration Compatibility", func(t *testing.T) {
		// åˆ›å»ºå®Œæ•´çš„åº”ç”¨é…ç½®
		cfg := &config.AppConfig{}
		
		// è®¾ç½®æ‰€æœ‰å¿…éœ€çš„å­—æ®µ
		cfg.Kafka.Brokers = []string{"localhost:9092"}
		cfg.Kafka.Topics.DeviceData = "device-data"
		cfg.Kafka.Producer.ClientID = "test-producer"
		cfg.Kafka.Producer.Timeout = 30 * time.Second
		cfg.Device.Simulator.SampleInterval = 1 * time.Second
		cfg.Device.Simulator.DeviceCount = 10
		cfg.Web.Port = 8080

		// éªŒè¯é…ç½®å¯ä»¥æ­£å¸¸ä½¿ç”¨
		if cfg.Kafka.Topics.DeviceData == "" {
			t.Fatal("DeviceData topic configuration failed")
		}
		if cfg.Device.Simulator.SampleInterval == 0 {
			t.Fatal("SampleInterval configuration failed")
		}

		t.Log("âœ… Configuration compatibility verified")
	})

	t.Run("Core Components Integration", func(t *testing.T) {
		// æµ‹è¯•æ ¸å¿ƒç»„ä»¶å¯ä»¥ååŒå·¥ä½œ
		metrics := NewPrometheusMetrics()
		pool := NewMemoryPool(1024, 10)
		defer pool.Close()

		// æ¨¡æ‹Ÿä¸€ä¸ªå·¥ä½œæµç¨‹
		start := time.Now()
		
		for i := 0; i < 100; i++ {
			// ä»å†…å­˜æ± è·å–ç¼“å†²åŒº
			buf := pool.Get()
			
			// åˆ›å»ºé›¶æ‹·è´ç¼“å†²åŒº
			zcBuf := NewZeroCopyBuffer(buf[:100])
			
			// æ‰§è¡Œé›¶æ‹·è´æ“ä½œ
			_ = zcBuf.String()
			_ = zcBuf.Bytes()
			
			// è®°å½•æŒ‡æ ‡
			metrics.IncrementCounter("operations", nil)
			metrics.RecordHistogram("operation_duration", float64(time.Since(start).Nanoseconds()), nil)
			
			// å½’è¿˜ç¼“å†²åŒº
			pool.Put(buf)
		}
		
		duration := time.Since(start)
		
		// éªŒè¯æŒ‡æ ‡è®°å½•
		if metrics.counters["operations"] != 100 {
			t.Fatalf("Expected 100 operations, got %f", metrics.counters["operations"])
		}
		
		t.Logf("âœ… Integrated workflow completed 100 operations in %v", duration)
	})

	t.Log("ğŸ‰ System integration readiness validated - All core components working correctly!")
}
